{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472edca1-79e3-432d-887f-8fcf8eceda7b",
   "metadata": {},
   "source": [
    "# The Anatomy of a Power Outage\n",
    "\n",
    "The U.S. Department of Energy (DOE) provides critical information about the status and impacts of energy sector disruptions through the **Environment for Analysis of Geo-Located Energy Information (EAGLE-I)** system, operated by Oak Ridge National Laboratory. EAGLE-I supports monitoring of energy infrastructure assets, reporting of power outages, visualization of threats to energy infrastructure, and coordination of emergency response and recovery efforts.\n",
    "\n",
    "Effectively responding to and restoring power during disasters depends on having timely, accurate, and actionable data.\n",
    "\n",
    "In this exercise, your goal is to learn about K-Means and how clustering data can help you characterize it. This activity is based on one of the exercises from a larger data bootcamp that seeks to identify the characteristics and causes of power outages in the United States.\n",
    "\n",
    "## Background \n",
    "\n",
    "K-Means is a machine learning algorithm used for clustering, which means grouping data points that are similar to each other. Instead of having predefined labels (as in classification), K-Means finds structure in the data on its own.\n",
    "- You choose a number of clusters, K.\n",
    "- The algorithm finds centers (called centroids) for those clusters.\n",
    "- Each data point is assigned to the cluster with the nearest centroid.\n",
    "- The centroids are updated until the clusters stabilize.\n",
    "\n",
    "In this lab, we‚Äôll use K-Means to group power outages based on their location (latitude and longitude) and time of year. This way, we can discover natural patterns, like whether outages cluster in certain regions or seasons.\n",
    "\n",
    "## Why Normalize the Data?\n",
    "\n",
    "Normalization (or scaling) is important because K-Means relies on distances to decide which points are similar. If one variable has a much larger numeric range than another, it can dominate the distance calculation.\n",
    "\n",
    "For example:\n",
    "- longitude values range roughly from -180 to 180.\n",
    "- Time of year (say, months of the year 1‚Äì12) has a different scale.\n",
    "\n",
    "\n",
    "If we don‚Äôt normalize, the clustering will be biased toward the feature with larger numbers.\n",
    "\n",
    "By normalizing, we put all features on a comparable scale so that location and time both contribute fairly to the clustering.\n",
    "\n",
    "## What is the best number of clusters to choose for a given set of data?\n",
    "\n",
    "### The Elbow Method\n",
    "- K-Means needs you to choose K, the number of clusters. But how do you know the best K?\n",
    "- For each choice of K, we can calculate the within-cluster sum of squares (WCSS), which measures how close the points are to their cluster centers.\n",
    "- As K increases, WCSS always goes down (more clusters = tighter groups).\n",
    "- The trick is to look for the elbow in the WCSS vs. K graph:\n",
    "  - At first, adding more clusters makes the WCSS drop a lot.\n",
    "  - After a certain point, the improvement slows down.\n",
    "  - That ‚Äúelbow‚Äù point is often a good choice for K.\n",
    "\n",
    "There are other tests too that you will find in the exercises in this notebook.\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "You must activate the code cells in the notebook below for the code to be used. In some cases, you are only reading functions into the Python interpreter, and they will not produce output until you call the function in another cell.\n",
    "\n",
    "To activate a cell, click on it, and then hold down \"Shift\" while also pressing \"Enter\" or \"Return\" on the keyboard.\n",
    "\n",
    "## Tips for Using Jupyter Notebooks and Python\n",
    "\n",
    "- If you get a pink error box after running a cell, scroll to the **bottom** of the error box to see what the main error is.  \n",
    "- Often, if you get an error box, it‚Äôs because you skipped activating a cell above your current cell.  \n",
    "- You can use an AI assistant to help you understand what the error message means‚Äîjust make sure to **paste both the code that generated the error and the entire error message** into the AI for the best explanation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808142fe",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "First, you will import all the Python packages you need for this project below.\n",
    "\n",
    "Python packages are collections of modules that provide reusable code for specific tasks, such as data analysis or parallel computing. You import them using the `import` statement so that you can access the tools and functions they contain in your script.\n",
    "\n",
    "### üü© **TODO**\n",
    "To run the cell below, click on it, and then press **Shift + Enter** (or **Shift + Return**). This is how you will run all the code cells inside a Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0330463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788499ae-548a-461d-af1a-37ef84f94051",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "In this lab, we‚Äôll use K-Means to group power outages based on their location (latitude and longitude) and time of year. This way, we can discover natural patterns, such as whether outages cluster in certain regions or seasons.\n",
    "\n",
    "## FIPS Codes and Census Regions\n",
    "\n",
    "The data provided in the EAGLE-I datasets includes FIPS codes for outages reported from different areas. FIPS codes are unique identifiers that describe specific geographic locations. The FIPS codes provided in the EAGLE-I dataset are county-level, which means they are five digits in total. The first two digits represent the state, and the last three identify the county within that state.\n",
    "\n",
    "More about FIPS codes: https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt\n",
    "\n",
    "During our data cleaning, we provided a dictionary that uses the two-digit FIPS code for states to identify the state of each outage. We also provided a numerical identifier for the census region of the U.S. where each outage occurs. We do this to have additional features available for use in our unsupervised learning later on. The dictionary provided in the script, along with a map displaying the U.S. census regions, is shown below for convenience.\n",
    "\n",
    "![Map with Census Regions Indicated](./Images/census_regions.gif)\n",
    "\n",
    "## Regions  1: Pacific 2: Mountain 3: West North Central 4: West South Central 5: East North Central\n",
    "\n",
    "## 6: East South Central 7: New England 8: Mid-Atlantic 9: South Atlantic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15de110-ec3f-400c-9761-fd484e643928",
   "metadata": {},
   "source": [
    "## Read in and Sort the Data\n",
    "\n",
    "The dataset we provide is lightly cleaned but includes records from all U.S. states, including Alaska and Hawaii.  \n",
    "The continental United States lies between approximately 66.95¬∞W and 124.67¬∞W longitude.  \n",
    "Because Python (and most geographic systems) represents western longitudes as negative, the continental U.S. spans roughly ‚Äì67 to ‚Äì125.  \n",
    "To exclude Alaska and Hawaii, we keep only data with longitude values greater than ‚Äì130 (i.e., east of 130¬∞W).  \n",
    "This ensures we focus on outages within the contiguous United States.  \n",
    "\n",
    "Functions are a useful structure that allows us to repeat the same process with different inputs.  \n",
    "We‚Äôll define one to read in and clean the outage data so it can be reused easily for other datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Hold down the **Shift** key as you click on the cell below to activate the function code.  \n",
    "Do the same for the next cell to call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5d5ae-1df5-44a4-9627-c759d1eb0212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a CSV file of power outage data, extracts the outage start year,\n",
    "# removes invalid longitude entries (<= -130), and returns the cleaned DataFrame.\n",
    "\n",
    "def get_data(filename=\"AllOutages\"):\n",
    "    data = pd.read_csv(f\"/anvil/projects/x-cis230270/data/kmeans_data/{filename}.csv\")\n",
    "    # Convert the text values in the OutageStart column into datetime objects that Pandas can understand and work with.\n",
    "    data['year'] = pd.to_datetime(data['OutageStart'])\n",
    "    data['year'] = data['year'].dt.year\n",
    "\n",
    "    drop_indices = data[data['Long']<=-130].index\n",
    "    data.drop(drop_indices, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752dd83-9760-4448-9f87-cb4b36608b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function and look at the first 200 rows.\n",
    "data = get_data()\n",
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356c46bf-314c-46ed-a6ad-0f3a2fd41384",
   "metadata": {},
   "source": [
    "## Understanding the Data Columns\n",
    "\n",
    "Here is what the column headings mean:\n",
    "\n",
    "- **State:** The name of the state where the outage was located.  \n",
    "- **FIPS:** The code that corresponds to the state and county where the outage occurred.  \n",
    "- **StateNum:** The state number extracted from the FIPS code.  \n",
    "- **Region:** The larger U.S. region where the outage was located. See the map at the top of this notebook for regional definitions.  \n",
    "- **Lat:** Latitude of the outage location.  \n",
    "- **Long:** Longitude of the outage location.  \n",
    "- **Month:** The month during which the outage took place (across any year in the dataset).  \n",
    "- **Month_Sin:** The month expressed as a cyclic component to help ensure outages occurring at the end of one year (e.g., December) and at the beginning of the next (e.g., January) are grouped together.  \n",
    "- **Month_Cos:** The second cyclic component used similarly to keep outages at the end and beginning of the year together.  \n",
    "- **OutageStart:** The date and time when the outage began.  \n",
    "- **OutageEnd:** The date and time when the outage ended.  \n",
    "- **OutageLength:** The duration of each outage, calculated for all customers in each county.  \n",
    "\n",
    "**Note:** The data cleaning script collected power outage data for every county in the U.S. at 15-minute intervals for all dates between 2016 and 2022. It defines an outage start as the first date and time when more than 10% of the total customers in a county are without power, and the outage length is calculated until the total number of customers without power falls below the 10% threshold.\n",
    "\n",
    "- **Sum:** Represents the average number of customers who were without power over the duration of each outage for each county.  \n",
    "- **Year:** The year in which the outage occurred.  \n",
    "\n",
    "As you can see, this dataset gives you many different opportunities to sort or aggregate the data.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "For a visual understanding of the data, run the cell below to display a histogram showing the total number of outages per region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d03a84-2e91-4c1e-aad7-3d0ca9db5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hold shift while clicking on the cell below to see a histogram of the total nubmer of outages in the data set for each region. \n",
    "\n",
    "# Count the number of outages per Region in all the data\n",
    "# This line counts the number of entries per region in the dataframe\n",
    "outages_in_data = data['Region'].value_counts().sort_index()\n",
    "\n",
    "\n",
    "# Plot the histogram (bar chart)\n",
    "plt.figure(figsize=(10,6))\n",
    "outages_in_data.plot(kind='bar')\n",
    "\n",
    "plt.title('Number of Outages from 2014-2022 by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Number of Outages')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92496951-8fbf-4c08-8d12-fbd1abb3cbe1",
   "metadata": {},
   "source": [
    "### Regions \n",
    " 1: Pacific 2: Mountain 3:West North Central 4: West South Central 5: East North Central 6: East South Central 7: New England\n",
    " 8: Mid-Atlantic 9:South Atlantic\n",
    "\n",
    "Right away, you can see that some regions report more power outages than others!  \n",
    "Perhaps this is due to differences in weather, population, or power infrastructure.  \n",
    "Clustering the outages by season and region can help us begin to understand their connection to weather.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "1. Which regions have the largest number of power outages?  \n",
    "2. Why might those regions have more outages than others?  \n",
    "Make are note for yourself with your thoughts about these questions.\n",
    "\n",
    "*(Scroll up to use the map if you need to see where the regions are located.)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaba03f-cb87-41eb-abc3-79db04301043",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "Let‚Äôs use K-Means to see whether power outages tend to cluster in regions that experience more severe weather. To do this rigorously, we would need to combine our outage data with a severe weather database‚Äîwhich is exactly what we do in the week-long Data Science Camp that uses this dataset. However, for this short exercise, we‚Äôll rely on our physical intuition and personal experience. For example, we know that in areas that are less arid, thunderstorms often occur during the summer and are known to cause damage to power lines.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise: Select the Features\n",
    "\n",
    "First, we need to select the data features we‚Äôll use for clustering.  \n",
    "Obvious choices from our dataset include **latitude**, **longitude**, and **month**.  \n",
    "Instead of using raw month values, we‚Äôll use the **Month_Sin** and **Month_Cos** components to represent the cyclical nature of time‚Äîthis helps the model recognize smooth transitions between December and January.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "In the cell below, replace `'REPLACE ME'` with the following column names to select your features:\n",
    "\n",
    "```\n",
    "'Long', 'Lat', 'Month_Sin', 'Month_Cos'\n",
    "```\n",
    "\n",
    "Then activate the cell by pressing **Shift + Return** (or **Shift + Enter**).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59b8f9-9e4d-4c08-9c06-48a5503ab7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Features to Cluster\n",
    "columns=['REPLACE ME', 'REPALCE ME', 'REPLACE ME', 'REPLACE ME']\n",
    "#Make a new data frame called 'X' with just those features\n",
    "X=data[columns]\n",
    "#drop any rows that contain not an number errors (nan)\n",
    "X=X.dropna()\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f2222-2fb2-496d-9a7d-61403dd28c02",
   "metadata": {},
   "source": [
    "### üü© **TODO**\n",
    "\n",
    "Read and understand why we need to **normalize the features**.\n",
    "\n",
    "Notice that the **Longitude** and **Latitude** values have magnitudes in the tens or hundreds, while the **Month_Sin** and **Month_Cos** components are much smaller‚Äîtypically between ‚Äì1 and 1.\n",
    "\n",
    "As we discussed earlier, K-Means clustering is a geometric algorithm: it measures how far points are from each other in multidimensional space to decide which cluster they belong to.\n",
    "\n",
    "If one feature (like **Longitude**) has values hundreds of times larger than another (like **Month_Sin**), it will dominate the distance calculation. This means K-Means will place its cluster centers closer to the features with the largest numerical scales, ignoring the smaller ones.\n",
    "\n",
    "To give all features an equal influence on the clustering, we need to rescale the data so that every feature has roughly the same magnitude‚Äîtypically by transforming them to have a mean of 0 and a standard deviation of 1.  \n",
    "This process is called **standardization** or **normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5980419-7de5-41be-929e-35e11c0f71e5",
   "metadata": {},
   "source": [
    "## Exercise: Normalize Features  \n",
    "\n",
    "Next, we‚Äôll normalize the data so that all features contribute equally to the clustering.  \n",
    "\n",
    "Your DataFrame is called **`X`**.  \n",
    "\n",
    "The line in the cell below imports the **StandardScaler** class from the scikit-learn library‚Äôs preprocessing module.  \n",
    "**StandardScaler** standardizes numerical data by removing the mean and scaling to unit variance (i.e., converting each feature into a z-score).  \n",
    "\n",
    "This means that after scaling, each feature (column) will have:  \n",
    "- a mean ‚âà 0  \n",
    "- a standard deviation ‚âà 1  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Activate the cell below to import the **StandardScaler** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488e1ec2-8a04-483f-a984-3aed3236eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb92e00-131e-4573-8a28-3b6166cde5d5",
   "metadata": {},
   "source": [
    "### üü© **TODO**\n",
    "\n",
    "Create an instance of the **StandardScaler** class called `scaler`.\n",
    "\n",
    "- Think of it as creating a tool that can learn how to scale your data.  \n",
    "- It will remember the mean and standard deviation of each column during fitting.\n",
    "\n",
    "In the cell below, type:\n",
    "\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "```\n",
    "\n",
    "Then activate the cell by pressing **Shift + Return** (or **Shift + Enter**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8362b-954a-4486-a9d2-8f3caa782384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO Create an instance of the StandardScaler class called \"scaler\". \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149851e-2e88-4fdb-afc0-d4ab79fa62e3",
   "metadata": {},
   "source": [
    "This line does two steps in one:\n",
    "\n",
    "- **`fit()`** ‚Äì calculates the mean and standard deviation for each column in **X**.  \n",
    "- **`transform()`** ‚Äì uses those values to standardize each data point using the formula:\n",
    "\n",
    "z = (x ‚àí Œº)/ œÉ\n",
    "\n",
    "where:  \n",
    "- **x** = original value  \n",
    "- **Œº** = column mean  \n",
    "- **œÉ** = column standard deviation  \n",
    "\n",
    "The result, **`norm_X`**, is a NumPy array of scaled values.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Activate the cell below to fit and transform the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58680fcc-89cb-416d-8e1b-36c5c471ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the data to normalize its vales. \n",
    "norm_X = scaler.fit_transform(X)\n",
    "\n",
    "print (norm_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508717cf-81c1-48a1-95bd-e0ce657e9c9e",
   "metadata": {},
   "source": [
    "You can see that the values of all these featires fall between -2 and 2, so when we use geometery to find the distance between each point for K-means, it is not skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9ce76-14e1-486c-95ff-06727c8d133f",
   "metadata": {},
   "source": [
    "## Methods of Determining K  \n",
    "\n",
    "### The Elbow Method  \n",
    "\n",
    "The **Within-Cluster Sum of Squares (WCSS)** measures how tightly the data points within each K-Means cluster are grouped around their respective centroids‚Äîin other words, how compact the clusters are.  \n",
    "\n",
    "The loop below runs K-Means for cluster counts from 1 to 29 and records the WCSS for each value of **k**.  \n",
    "This allows us to see how the compactness of the clusters changes as the number of clusters increases.  \n",
    "In the resulting graph, look for a turning point or ‚Äúelbow‚Äù ‚Äî the point after which adding more clusters yields diminishing improvements in compactness.  \n",
    "This elbow helps guide our choice for the most appropriate number of clusters.  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Activate the cell below to run the loop and generate the WCSS values. Note the X-axis value where you think the \"elbow\" of the plot is located, you will need it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8363f1cc-eaf3-410e-91d8-c9efb7ee05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "from sklearn.cluster import KMeans\n",
    "for i in range(1, 30):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(norm_X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "plt.plot(range(1,30), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "# plt.savefig(f\"Plots/elbow-{filename}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb70fb5-b776-4c2a-ba86-71bc78414d72",
   "metadata": {},
   "source": [
    "## Davies-Bouldin Index  \n",
    "\n",
    "Another method we can use that provides a more explicit numerical value than the Elbow Method is the **Davies-Bouldin Index**.  \n",
    "The Davies-Bouldin Index measures similarity between clusters and their most similar cluster by taking the ratio of within-cluster distances to between-cluster distances.  \n",
    "A lower Davies-Bouldin score indicates better clustering performance.  \n",
    "\n",
    "**Scikit-learn‚Äôs Davies-Bouldin Index documentation:**  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.davies_bouldin_score.html#sklearn.metrics.davies_bouldin_score  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Activate the cell below to run a Davies-Bouldin Index scoring on your data.  \n",
    "Which value of **K** gives the lowest score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0038a0e-32d7-430b-a2a6-70601e657ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "for k in range(2,20):\n",
    "    clusterer = KMeans(n_clusters=k, random_state=10,n_init='auto')\n",
    "    labels = clusterer.fit_predict(norm_X)\n",
    "\n",
    "    print(f\"{k} clusters the Davies-Bouldin Score is: {davies_bouldin_score(norm_X, labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3f199-7ba4-4186-815c-3589b68e61b4",
   "metadata": {},
   "source": [
    "## Calinski-Harabasz Index  \n",
    "\n",
    "The final method we‚Äôll use to determine the best **K** is the **Calinski-Harabasz Index**.  \n",
    "This score evaluates the ratio of between-cluster dispersion to within-cluster dispersion.  \n",
    "It is also known as the **Variance Ratio Criterion**.  \n",
    "For this metric, **higher values** indicate better-defined clusters.  \n",
    "\n",
    "**Scikit-learn‚Äôs Calinski-Harabasz Index documentation:**  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html#sklearn.metrics.calinski_harabasz_score  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Activate the cell below to generate the Calinski-Harabasz scores for your data across different values of **K**.  \n",
    "Which value of **K** gives the highest score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2611d-8e83-45d9-8b30-c554cfe347e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "for k in range(2,20):\n",
    "    clusterer = KMeans(n_clusters=k, random_state=10,n_init='auto')\n",
    "    labels = clusterer.fit_predict(norm_X)\n",
    "\n",
    "    print(f\"{k} clusters the Calinski-Harabasz Score is: {calinski_harabasz_score(norm_X, labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b6ce8-2393-4b84-a5bc-6a2694f2fa55",
   "metadata": {},
   "source": [
    "# Choosing K and Plotting the Clusters  \n",
    "\n",
    "After running through the different methods of determining **K** above, you should now have some good ideas about which value(s) of **K** might be best.  \n",
    "Now you can test them and see what the clustering looks like when translated to a graph.  \n",
    "\n",
    "Below is a clustering function that plots our data with color-coded clusters based on K-Means.  \n",
    "Try experimenting with the value of **K**, or even with which columns you plot (be sure to use only **2 or 3 columns**), to see what patterns you can find.  \n",
    "\n",
    "*(Note that changing the columns only changes how the points are displayed‚Äîit does not change which features are used for clustering. Those were set when we created `norm_X` in the Elbow Method.)*  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "In the next cell:  \n",
    "1. Enter the **K** value you chose based on your earlier analysis in place of \"Replace_Me\".(Do not put the nubmer in quotes) \n",
    "2. Activate the cell to load that value of **K** for the next set of exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e61d39-9eee-4809-aff0-d0f51895afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Determine the best k to use here\n",
    "k = Replace_Me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9946cd0-d9aa-4d3d-9d9f-c6e3980407c8",
   "metadata": {},
   "source": [
    "### Plotting the Clusters\n",
    "\n",
    "Below is a clustering function that plots our data with color-coded clusters based on K-Means.\n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "1. Try experimenting with different values of **K** to see how the plot changes.  \n",
    "2. Try removing the **Month** column and creating a 2D plot using only **Latitude** and **Longitude** to observe any patterns.  \n",
    "   *(Note: Changing the columns only affects how the points are plotted‚Äîit does not change which features are used for clustering, since those were set when we created `norm_X` in the Elbow Method.)*  \n",
    "3. Reset the plot to use `['Long', 'Lat', 'Month']` before continuing to the next section.  \n",
    "4. Activate the cell below to load the function, then run the following cell to call and use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f0f99-c8f1-4820-ac12-6d7d0e535439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(data, norm_X, k=3, columns=['Region', 'Month', 'OutageLength'], scale=False):\n",
    "    dims = len(columns)\n",
    "    if dims > 3 or dims < 2:\n",
    "        print(\"Should be looking at 2 or 3 features, change number of columns evaluated.\")\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    data['clusters'] = kmeans.fit_predict(norm_X)\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 14))\n",
    "\n",
    "    # 3d clustering\n",
    "    if dims == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    # 2d clustering\n",
    "    else:\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "    for i in range(k):\n",
    "        cluster_data = data[data['clusters'] == i]\n",
    "        if scale:\n",
    "            sizes = cluster_data['Sum']\n",
    "        else:\n",
    "            sizes = 10\n",
    "        if dims == 3:\n",
    "            ax.scatter(cluster_data[columns[0]], cluster_data[columns[1]], cluster_data[columns[2]], label=f'Cluster {i + 1}', edgecolor='black', alpha=0.5, s=sizes/30)\n",
    "        else:\n",
    "            ax.scatter(cluster_data[columns[0]], cluster_data[columns[1]], label=f'Cluster {i + 1}', edgecolor='black', alpha=0.5, s=sizes/30)\n",
    "\n",
    "    ax.set_title('Clusters of Outages')\n",
    "    ax.set_xlabel(columns[0])\n",
    "    ax.set_ylabel(columns[1])\n",
    "    if dims == 3:\n",
    "        ax.set_zlabel(columns[2])\n",
    "    lgnd = ax.legend(markerscale=1)\n",
    "    for handle in lgnd.legend_handles:\n",
    "        handle.set_sizes([24.0])\n",
    "    return data, kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736d942-b065-4bce-b052-b5a90ad10b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range_mask = (data['Month'] >= 6.0) & (data['Month'] < 12.0)\n",
    "new_data, centroids = cluster(data, norm_X, k, columns=['Long', 'Lat','Month'], scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7986744-e746-4eea-86e9-c087dafd2ee1",
   "metadata": {},
   "source": [
    "## Helpers  \n",
    "\n",
    "In the following section, you are provided with several helper functions that will assist you in your workflow and analysis.\n",
    "\n",
    "Below is a brief explanation of these functions and what they can be used for:\n",
    "\n",
    "- **circular_toy**  \n",
    "  - This function takes a time-of-year (**toy**) value from an outage entry and calculates two components of that value using basic trigonometric functions.  \n",
    "\n",
    "- **plot_range_slice**  \n",
    "  - This function takes your outage data, the number of clusters you‚Äôre separating it into, and a lower and upper range of time of year, then plots a 2D slice of the data from that time frame.  \n",
    "  - You can optionally choose to scale the points by how many people are affected by each outage.  \n",
    "\n",
    "- **revert_comp**  \n",
    "  - This function takes two components of a time of year and converts them back to their original value.  \n",
    "\n",
    "- **centroid_toy**  \n",
    "  - This function takes the centroids of all clusters and the scaler used to normalize the original data, and converts the centroids to a time-of-year value for each cluster.  \n",
    "\n",
    "- **concat_helper**  \n",
    "  - This function takes two file names, each containing a CSV of outages, and concatenates them into a single CSV file.  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Press **Shift + Return** (or **Shift + Enter**) in each of the following cells to activate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93893cd-4051-46ca-bc84-46a20537d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ee03a-9eb4-4b3f-a529-de19ebf8b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_column(df, column, filter_val):\n",
    "    mask = (df[column] == filter_val)\n",
    "    return df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e977d0c-9f69-48d9-ab39-7bde3b22aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circular_toy(toy):\n",
    "\n",
    "    x = (2*math.pi*(toy-1)) / 12\n",
    "    toy_sin = math.sin(x)\n",
    "    toy_cos = math.cos(x)\n",
    "    \n",
    "    return toy_sin, toy_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc6ea2-4686-4317-aa7d-22c50f63dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_range_slice(data, k, lower, upper, scale=False):\n",
    "    low_sin, low_cos = circular_toy(lower)\n",
    "    up_sin, up_cos = circular_toy(upper)\n",
    "    # print(f'Low Sin: {low_sin}, Low Cos: {low_cos}, High Sin: {up_sin}, High Cos: {up_cos}')\n",
    "    # date_mask = (new_data['Month'] >= lower) & (new_data['Month'] < upper)\n",
    "    date_mask = None\n",
    "    if lower < upper:\n",
    "        date_mask = (data['Month'] >= lower) & (data['Month'] < upper)\n",
    "    else:\n",
    "        date_mask = (data['Month'] >= lower) | (data['Month'] < upper)\n",
    "    masked_data = data[date_mask]\n",
    "    # print(masked_data)\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    ax = fig.add_subplot()\n",
    "    for i in range(k):\n",
    "        cluster_data = masked_data[masked_data['clusters'] == i]\n",
    "        if scale:\n",
    "            sizes = cluster_data['Sum']/30\n",
    "        else:\n",
    "            sizes = 100\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        ax.scatter(cluster_data['Long'], cluster_data['Lat'], label=f'Cluster {i+1}', edgecolor='black', alpha=0.5, s=sizes)\n",
    "    \n",
    "    ax.set_title(f'Clusters of Outages for Time Between Month {lower} and {upper}')\n",
    "    lgnd = ax.legend(markerscale=1)\n",
    "    for handle in lgnd.legend_handles:\n",
    "        handle.set_sizes([24.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bcc43-6db5-49bd-ad6b-3b3d82ea252f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_toy(centroids, scaler):\n",
    "    inversed_centroids = scaler.inverse_transform(centroids)\n",
    "    for i in range(0, len(inversed_centroids)):\n",
    "        print(f\"Centroid for Cluster {i+1} occurs during time of year: {revert_comp(inversed_centroids[i][2], inversed_centroids[i][3])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ed4de2-385b-42b9-8cbc-b539afa3706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def revert_comp(sin, cos):\n",
    "    theta = math.atan2(sin, cos)\n",
    "\n",
    "    x = 1 + (12 * theta) / (2 * math.pi)\n",
    "\n",
    "    if x < 1:\n",
    "        x += 12\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3d4297-4a79-4ca6-80ee-7f0ee1507293",
   "metadata": {},
   "source": [
    "# Analysis  \n",
    "\n",
    "## When Are the Outage Clusters Centered  \n",
    "\n",
    "We can use one of our helper functions, `centroid_toy`, to find the time of year where each cluster is centered.  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Press **Shift + Return** (or **Shift + Enter**) on the cells below to call the `centroid_toy` function.\n",
    "\n",
    "**What months are represented by the cluster centroids?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980c2ef-c6eb-4b0e-b34e-98082fe55d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_toy(centroids, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2fcb76-0a0f-4c11-88f5-4beb015735ce",
   "metadata": {},
   "source": [
    "Another one of our helper functions, `plot_range_slice`, can be used to view our data in a 2D slice during a specified range of time.  \n",
    "We can use this to evaluate clusters in different regions during times of the year that we‚Äôre interested in.  \n",
    "\n",
    "*(Note: You can specify ranges where the lower bound is a month following the upper bound, and the range will wrap around the year.  \n",
    "For example, `lower = 12` and `upper = 2` will show data from December through February.)*  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Press **Shift + Return** (or **Shift + Enter**) on the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa11dd-bddb-437c-8898-c2b8619f9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range_slice(data, k, 1.0, 12.0, scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a724e-8520-4afc-ab05-b53ec7e0d733",
   "metadata": {},
   "source": [
    "Here we use the `plot_range_slice` function to create graphs for each month to see how our data changes and clusters throughout the year.  \n",
    "\n",
    "---\n",
    "\n",
    "### üü© **TODO**\n",
    "\n",
    "Use the `plot_range_slice` function to generate plots for each month.  \n",
    "Observe how the clusters shift or change across different times of the year.  \n",
    "Consider:  \n",
    "- Do certain regions experience more outages during specific months?  \n",
    "- Do cluster patterns align with known seasonal weather trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd349dd8-4097-444d-9557-0bc5548ff2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 13):\n",
    "    plot_range_slice(new_data, 7, i, i + 1, scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003f321-8d81-4ac4-bc6c-4444eb56c811",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "\n",
    "### Step 1: Refine your clusters\n",
    "1. Test and adjust the number of power outage clusters based on the results and tests in the notebook\n",
    "\n",
    "### Step 2: Analyze and characterize the results\n",
    "- This is the process of determining what story the data and results tell you.\n",
    "Pick a region of\n",
    "Questions to consider:\n",
    "- When are the centers of those clusters for each region?\n",
    "  - Use your plot slices and the results of the centroid_toy to answer\n",
    "  - What sever weather happens in those the regions during the month where the clsuter is centered?\n",
    "    \n",
    "- What are the general characteristics of the power outages in those clusters?\n",
    "  - Are they generally long or short outages (large or small circles)?\n",
    "  - How many customers are impacted on average?\n",
    "- Are there regions or states that mostly share clusters or that mostly do not share clusters?\n",
    "  - Why might that be in each case?\n",
    "  - Form some hypotheses about the possible relationships to:\n",
    "    - Regional climate and weather\n",
    "    - The time of year\n",
    "    - The severity of the power outages\n",
    "- How well do you trust the cluster results?\n",
    "\n",
    "\n",
    "# üü© **Final TODO**\n",
    "Please fill out this form with your analysis to get credit for this challenge: https://forms.gle/zajaAVTCQy1PCp5y6\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
